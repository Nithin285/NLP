{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNCKqjP2hQ8nzJgChiTuvpj",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Nithin285/NLP/blob/main/NLP_LAB_Assignment_01.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c310d871"
      },
      "source": [
        "# Task\n",
        "Process a short paragraph of text by loading it into a string, counting its words, and converting it to lowercase, after installing NLTK and spaCy."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "14585f68"
      },
      "source": [
        "## Install and Import Libraries\n",
        "\n",
        "### Subtask:\n",
        "Install NLTK and spaCy using pip, then import them into the notebook.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b6807461"
      },
      "source": [
        "**Reasoning**:\n",
        "Install NLTK and spaCy libraries using pip, then import them into the notebook.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "130d6900",
        "outputId": "f6a3f316-31de-463a-9b18-efbc56e1fe9b"
      },
      "source": [
        "import sys\n",
        "!{sys.executable} -m pip install nltk spacy\n",
        "\n",
        "import nltk\n",
        "import spacy\n",
        "\n",
        "print(\"NLTK and spaCy installed and imported successfully.\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: nltk in /usr/local/lib/python3.12/dist-packages (3.9.1)\n",
            "Requirement already satisfied: spacy in /usr/local/lib/python3.12/dist-packages (3.8.11)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.12/dist-packages (from nltk) (8.3.1)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.12/dist-packages (from nltk) (1.5.3)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.12/dist-packages (from nltk) (2025.11.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from nltk) (4.67.1)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.12/dist-packages (from spacy) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from spacy) (1.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.12/dist-packages (from spacy) (1.0.15)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.12/dist-packages (from spacy) (2.0.13)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.12/dist-packages (from spacy) (3.0.12)\n",
            "Requirement already satisfied: thinc<8.4.0,>=8.3.4 in /usr/local/lib/python3.12/dist-packages (from spacy) (8.3.10)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.12/dist-packages (from spacy) (1.1.3)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.12/dist-packages (from spacy) (2.5.2)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.12/dist-packages (from spacy) (2.0.10)\n",
            "Requirement already satisfied: weasel<0.5.0,>=0.4.2 in /usr/local/lib/python3.12/dist-packages (from spacy) (0.4.3)\n",
            "Requirement already satisfied: typer-slim<1.0.0,>=0.3.0 in /usr/local/lib/python3.12/dist-packages (from spacy) (0.20.0)\n",
            "Requirement already satisfied: numpy>=1.19.0 in /usr/local/lib/python3.12/dist-packages (from spacy) (2.0.2)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.12/dist-packages (from spacy) (2.32.4)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.12/dist-packages (from spacy) (2.12.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from spacy) (3.1.6)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from spacy) (75.2.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from spacy) (25.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.41.4 in /usr/local/lib/python3.12/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (2.41.4)\n",
            "Requirement already satisfied: typing-extensions>=4.14.1 in /usr/local/lib/python3.12/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (4.15.0)\n",
            "Requirement already satisfied: typing-inspection>=0.4.2 in /usr/local/lib/python3.12/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.4.2)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2025.11.12)\n",
            "Requirement already satisfied: blis<1.4.0,>=1.3.0 in /usr/local/lib/python3.12/dist-packages (from thinc<8.4.0,>=8.3.4->spacy) (1.3.3)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.12/dist-packages (from thinc<8.4.0,>=8.3.4->spacy) (0.1.5)\n",
            "Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in /usr/local/lib/python3.12/dist-packages (from weasel<0.5.0,>=0.4.2->spacy) (0.23.0)\n",
            "Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in /usr/local/lib/python3.12/dist-packages (from weasel<0.5.0,>=0.4.2->spacy) (7.5.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->spacy) (3.0.3)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.12/dist-packages (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.4.2->spacy) (2.0.1)\n",
            "NLTK and spaCy installed and imported successfully.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "96d17c57"
      },
      "source": [
        "## Load Text\n",
        "\n",
        "### Subtask:\n",
        "Load a short paragraph of text into a Python string variable. You can either copy and paste the text directly or load it from a file.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "77ab7f16"
      },
      "source": [
        "**Reasoning**:\n",
        "To complete the subtask of loading a short paragraph into a Python string, I will create a code block that defines a variable named `paragraph_text` and assigns a sample paragraph to it.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "02950f79",
        "outputId": "299019c3-93a5-40f0-f898-63379aa6b74d"
      },
      "source": [
        "paragraph_text = \"NLTK (Natural Language Toolkit) is a powerful Python library for working with human language data. It provides easy-to-use interfaces to over 50 corpora and lexical resources such as WordNet, along with a suite of text processing libraries for classification, tokenization, stemming, tagging, parsing, and semantic reasoning. SpaCy, on the other hand, is an industrial-strength natural language processing library built for efficiency and speed. It's designed specifically for production use and helps you build applications that process large volumes of text.\"\n",
        "print(\"Paragraph loaded successfully.\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Paragraph loaded successfully.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e51890fd"
      },
      "source": [
        "## Count Words\n",
        "\n",
        "### Subtask:\n",
        "Count the total number of words in the loaded text.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6ac8764f"
      },
      "source": [
        "To count the words in the `paragraph_text`, I'll start by defining a preprocessing step to remove punctuation, ensuring that each word is counted without being affected by surrounding symbols. Following this, I will split the cleaned text into individual words using whitespace as a delimiter. Finally, I'll count the number of words in the resulting list and display this count. This approach provides a clear and accurate word count for the given paragraph."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "655d462c"
      },
      "source": [
        "**Reasoning**:\n",
        "I will clean the paragraph text by removing punctuation and then split it into words to get an accurate word count. Finally, I will print the total word count.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e7291d4c",
        "outputId": "e606365e-f6c3-4b95-fecd-779f85c4589e"
      },
      "source": [
        "import re\n",
        "\n",
        "# Remove punctuation from the text\n",
        "cleaned_text = re.sub(r'[\\W_]+', ' ', paragraph_text)\n",
        "\n",
        "# Split the cleaned text into words and count them\n",
        "words = cleaned_text.split()\n",
        "word_count = len(words)\n",
        "\n",
        "# Print the word count\n",
        "print(f\"Total word count: {word_count}\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total word count: 84\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "acf49b07"
      },
      "source": [
        "## Convert to Lowercase\n",
        "\n",
        "### Subtask:\n",
        "Convert the entire text to lowercase using Python's built-in string methods.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7d33d6cc"
      },
      "source": [
        "**Reasoning**:\n",
        "To convert the `paragraph_text` to lowercase, I will use Python's built-in `.lower()` string method and store the result in a new variable called `lowercase_text`, then print it to verify the conversion.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8e3350b2",
        "outputId": "a894853f-a94c-433c-ad5a-eec47f479f8e"
      },
      "source": [
        "lowercase_text = paragraph_text.lower()\n",
        "print(lowercase_text)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "nltk (natural language toolkit) is a powerful python library for working with human language data. it provides easy-to-use interfaces to over 50 corpora and lexical resources such as wordnet, along with a suite of text processing libraries for classification, tokenization, stemming, tagging, parsing, and semantic reasoning. spacy, on the other hand, is an industrial-strength natural language processing library built for efficiency and speed. it's designed specifically for production use and helps you build applications that process large volumes of text.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "268814e0"
      },
      "source": [
        "## Final Task\n",
        "\n",
        "### Subtask:\n",
        "Review the processed text and word count, and ensure all requirements are met before preparing for submission.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6a7b6565"
      },
      "source": [
        "## Summary:\n",
        "\n",
        "### Data Analysis Key Findings\n",
        "\n",
        "*   **Library Setup**: NLTK and spaCy libraries were successfully installed (or found to be pre-installed) and imported without errors, confirming the environment was ready for NLP tasks.\n",
        "*   **Text Loading**: A short paragraph of text was successfully loaded into a Python string variable named `paragraph_text`.\n",
        "*   **Word Count**: After removing punctuation, the loaded paragraph was found to contain 84 words. The cleaning process used regular expressions to handle non-alphanumeric characters.\n",
        "*   **Lowercase Conversion**: The entire `paragraph_text` was successfully converted to its lowercase equivalent using Python's built-in string methods.\n",
        "\n",
        "### Insights or Next Steps\n",
        "\n",
        "*   The current word count method handles punctuation removal, but for more sophisticated analysis, consider normalizing numbers or handling hyphenated words (e.g., \"easy-to-use\" counted as one or two words).\n",
        "*   With NLTK and spaCy installed, the processed lowercase text can now be used for further advanced NLP tasks such as tokenization, lemmatization, part-of-speech tagging, or named entity recognition.\n"
      ]
    }
  ]
}